{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Immunotherapy Response based on RNA-Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data and gene pathways\n",
    "TCGA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tpm = pd.read_csv(\"data/tcga_sample/expression.tsv\", sep=\"\\t\")\n",
    "survival = pd.read_csv(\"data/tcga_sample/survival.tsv\", sep=\"\\t\", skiprows=1, header=None)\n",
    "meta = pd.read_csv(\"data/tcga_sample/metadata.tsv\", sep=\"\\t\", skiprows=1, header=None)\n",
    "cytokines = pd.read_csv(\"data/genes.cytokine_immune.txt\", skiprows=2, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the TPM values for cytokines pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use cytokine expression\n",
    "tpm = tpm.reindex(cytokines.iloc[:,0].unique(), axis='columns')\n",
    "tpm = tpm.dropna(axis=1)\n",
    "\n",
    "# perform quantile normalization\n",
    "# https://stackoverflow.com/questions/37935920/quantile-normalization-on-pandas-dataframe\n",
    "tpm /= np.max(np.abs(tpm),axis=0) # scale between [0,1]\n",
    "rank_mean = tpm.stack().groupby(tpm.rank(method='first').stack().astype(int)).mean()\n",
    "tpm = tpm.rank(method='min').stack().astype(int).map(rank_mean).unstack()\n",
    "\n",
    "# convert pandas df to np array\n",
    "tpm = tpm.values\n",
    "survival = survival.iloc[:,1:3].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "VALIDATION_SPLIT = 0.8\n",
    "\n",
    "# indices = np.arange(tpm.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# # tpm = tpm[indices]\n",
    "# labels = surv_time[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * tpm.shape[0])\n",
    "\n",
    "x_train = tpm[:num_validation_samples]\n",
    "y_train = survival[:num_validation_samples]\n",
    "x_val = tpm[num_validation_samples:]\n",
    "y_val = survival[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def negative_log_partial_likelihood_loss(regularization):\n",
    "    #Wrapper function for the negative logg partial likelihood loss function\n",
    "    \n",
    "    def loss(y_true, risk):\n",
    "        return negative_log_partial_likelihood(y_true, risk, regularization)\n",
    "    return loss\n",
    "\n",
    "def negative_log_partial_likelihood(censor, risk, regularization):\n",
    "    \"\"\"Return the negative log-partial likelihood of the prediction\n",
    "    y_true contains the survival time\n",
    "    risk is the risk output from the neural network\n",
    "    censor is the vector of inputs that are censored\n",
    "    regularization is the regularization constant (not used currently)\n",
    "    \n",
    "    Uses the Keras backend to perform calculations\n",
    "    \n",
    "    Sorts the surv_time by sorted reverse time\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate negative log likelihood from estimated risk\n",
    "    K.print_tensor(censor)\n",
    "    K.print_tensor(risk)\n",
    "    hazard_ratio = K.exp(risk)\n",
    "    log_risk = K.log(tf.cumsum(hazard_ratio)) # cumsum on sorted surv time accounts for concordance\n",
    "    uncensored_likelihood = risk - log_risk\n",
    "    censored_likelihood = uncensored_likelihood * censor\n",
    "    num_observed_events = K.sum(censor)\n",
    "    neg_likelihood = - K.sum(censored_likelihood) / tf.cast(num_observed_events, tf.float32)\n",
    "    return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(123)\n",
    "# negative_log_partial_likelihood(y_train, np.random.rand(y_train.shape[0], 2), 0).eval(session=K.get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00770439, 0.00770439, 0.09777909, 0.08354589, 0.14483557,\n",
       "         0.03750752, 0.06533576, 0.09507118, 0.10876231, 0.13890046,\n",
       "         0.15091908, 0.12206171, 0.48187445, 0.00770439, 0.00770439,\n",
       "         0.00770439, 0.16835241, 0.11855542, 0.04477338, 0.04356357,\n",
       "         0.08782074, 0.08175386, 0.16017084, 0.08354589, 0.36083847,\n",
       "         0.10876231, 0.08577805, 0.05328115, 0.05555696, 0.05104396,\n",
       "         0.07452724],\n",
       "        [0.00770439, 0.10876231, 0.00770439, 0.05433715, 0.01549208,\n",
       "         0.06885928, 0.00770439, 0.03144983, 0.0427311 , 0.00770439,\n",
       "         0.05784024, 0.05937713, 0.00770439, 0.00770439, 0.00770439,\n",
       "         0.00770439, 0.03144983, 0.02375978, 0.06885928, 0.05104396,\n",
       "         0.06533576, 0.09256924, 0.05328115, 0.04726486, 0.02430823,\n",
       "         0.03998144, 0.06391589, 0.0357079 , 0.02287782, 0.04186489,\n",
       "         0.05937713],\n",
       "        [0.00770439, 0.00770439, 0.04644952, 0.0125152 , 0.06056726,\n",
       "         0.06533576, 0.09777909, 0.04186489, 0.03750752, 0.00770439,\n",
       "         0.01385125, 0.05784024, 0.08782074, 0.00770439, 0.00770439,\n",
       "         0.00770439, 0.02985164, 0.04556465, 0.0255335 , 0.04726486,\n",
       "         0.03300299, 0.00923611, 0.02430823, 0.07452724, 0.08354589,\n",
       "         0.03465731, 0.02611727, 0.02145167, 0.0502553 , 0.04477338,\n",
       "         0.03998144],\n",
       "        [0.13890046, 0.00770439, 0.00770439, 0.01908645, 0.13392274,\n",
       "         0.04644952, 0.0793616 , 0.06391589, 0.06885928, 0.11175108,\n",
       "         0.36083847, 0.06691886, 0.00770439, 0.00770439, 0.00770439,\n",
       "         0.00770439, 0.03878618, 0.01049853, 0.00770439, 0.0125152 ,\n",
       "         0.00923611, 0.08782074, 0.04073407, 0.0125152 , 0.07228801,\n",
       "         0.09256924, 0.01622714, 0.01795834, 0.03878618, 0.01108299,\n",
       "         0.12903708],\n",
       "        [0.12206171, 0.00770439, 0.08577805, 0.02192631, 0.02430823,\n",
       "         0.08577805, 0.00770439, 0.02430823, 0.04477338, 0.12903708,\n",
       "         0.05433715, 0.07715581, 0.00770439, 0.00770439, 0.00770439,\n",
       "         0.00770439, 0.0502553 , 0.04477338, 0.02082614, 0.05937713,\n",
       "         0.0427311 , 0.08577805, 0.00770439, 0.05104396, 0.03998144,\n",
       "         0.04477338, 0.07715581, 0.0275692 , 0.01385125, 0.02611727,\n",
       "         0.05433715]]), array([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_iter(data, labels, batch_size, shuffle=True, isValidationSet=False):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "\n",
    "    # Sorts the batches by survival time\n",
    "    def data_generator():\n",
    "        data_size = len(data)\n",
    "        while True:\n",
    "            # Sample from the dataset for each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                X, y = shuffled_data[start_index: end_index], shuffled_labels[start_index: end_index]\n",
    "                \n",
    "                # Sort X and y by survival time in each batch\n",
    "                idx = np.argsort(abs(y[:,0]))[::-1]\n",
    "                X = X[idx, :]\n",
    "                y = y[idx, 1].reshape(-1,1) # sort by survival time and take censored data\n",
    "\n",
    "                # reshape for matmul\n",
    "                y = y.reshape(-1,1) #reshape to [n, 1] for matmul\n",
    "                \n",
    "                yield X, y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()\n",
    "\n",
    "train_steps, train_batches = batch_iter(x_train, y_train, 5)\n",
    "next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 2.4061 - acc: 0.3031 - val_loss: 1.9918 - val_acc: 0.1500\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 2.3646 - acc: 0.3281 - val_loss: 1.9887 - val_acc: 0.1500\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.4264 - acc: 0.3031 - val_loss: 1.9831 - val_acc: 0.1500\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.4286 - acc: 0.3083 - val_loss: 1.9793 - val_acc: 0.1500\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.3701 - acc: 0.2875 - val_loss: 1.9774 - val_acc: 0.1500\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 2.3496 - acc: 0.2729 - val_loss: 1.9774 - val_acc: 0.1500\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.3580 - acc: 0.2646 - val_loss: 1.9772 - val_acc: 0.1500\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.3559 - acc: 0.2573 - val_loss: 1.9818 - val_acc: 0.1500\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 2.3148 - acc: 0.2573 - val_loss: 1.9887 - val_acc: 0.1500\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 2.3302 - acc: 0.2698 - val_loss: 1.9964 - val_acc: 0.1500\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.3170 - acc: 0.2656 - val_loss: 2.0063 - val_acc: 0.1500\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.3264 - acc: 0.2542 - val_loss: 2.0199 - val_acc: 0.1500\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 2.2830 - acc: 0.2771 - val_loss: 2.0363 - val_acc: 0.1500\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.3278 - acc: 0.2938 - val_loss: 2.0575 - val_acc: 0.1500\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.2338 - acc: 0.2302 - val_loss: 2.0793 - val_acc: 0.1500\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.2638 - acc: 0.2771 - val_loss: 2.1070 - val_acc: 0.1500\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.1827 - acc: 0.2771 - val_loss: 2.1447 - val_acc: 0.1500\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.1997 - acc: 0.3198 - val_loss: 2.1823 - val_acc: 0.1500\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 2.2631 - acc: 0.2969 - val_loss: 2.2160 - val_acc: 0.1000\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.2551 - acc: 0.2958 - val_loss: 2.2360 - val_acc: 0.1000\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=x_train.shape[1], name=\"input\"))\n",
    "model.add(Dense(64, activation='relu', name=\"dense_1\"))\n",
    "model.add(Dropout(0.25, name=\"dropout_1\"))\n",
    "model.add(Dense(64, activation='relu', name=\"dense_2\"))\n",
    "model.add(Dense(1, activation='linear', name=\"output\"))\n",
    "\n",
    "opt = Adam(lr=0.001)\n",
    "\n",
    "model_loss = negative_log_partial_likelihood_loss(0)\n",
    "\n",
    "model.compile(optimizer=opt, loss=model_loss, metrics=['accuracy']) # Accuracy is meaningless in this case, only look at loss\n",
    "\n",
    "train_steps, train_batches = batch_iter(x_train, y_train, BATCH_SIZE)\n",
    "valid_steps, valid_batches = batch_iter(x_val, y_val, BATCH_SIZE)\n",
    "history = model.fit_generator(train_batches, train_steps, epochs=20, validation_data=valid_batches, validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13205379],\n",
       "       [-0.41463694],\n",
       "       [-0.55275315],\n",
       "       [-0.04706205],\n",
       "       [-0.5039119 ],\n",
       "       [ 0.08584359],\n",
       "       [-0.10762785],\n",
       "       [ 0.04295709],\n",
       "       [ 0.12689504],\n",
       "       [-1.5880321 ],\n",
       "       [ 0.4125125 ],\n",
       "       [-0.16764246],\n",
       "       [-0.2638064 ],\n",
       "       [-2.3235643 ],\n",
       "       [-0.4737324 ],\n",
       "       [-0.37796897],\n",
       "       [-0.3604821 ],\n",
       "       [-0.02432366],\n",
       "       [-1.087952  ],\n",
       "       [ 0.4487742 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(x_val)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "\n",
    "predictions_time = np.exp(predictions)\n",
    "concordance_index(y_val[:,0], predictions_time, y_val[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1411697 ],\n",
       "       [0.66058004],\n",
       "       [0.5753636 ],\n",
       "       [0.9540282 ],\n",
       "       [0.6041626 ],\n",
       "       [1.0896358 ],\n",
       "       [0.89796174],\n",
       "       [1.0438931 ],\n",
       "       [1.1352979 ],\n",
       "       [0.2043273 ],\n",
       "       [1.5106084 ],\n",
       "       [0.84565616],\n",
       "       [0.76812226],\n",
       "       [0.09792393],\n",
       "       [0.62267387],\n",
       "       [0.6852518 ],\n",
       "       [0.6973401 ],\n",
       "       [0.9759698 ],\n",
       "       [0.33690578],\n",
       "       [1.5663909 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 158,  247,   56,  284,   77,   65,  955,  273,  214,  271, 3617,\n",
       "       3056,  218,  178,  202,   55,  322,  237, 3767, 2292])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
