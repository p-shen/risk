{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Immunotherapy Response based on RNA-Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data and gene pathways\n",
    "TCGA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tpm = pd.read_csv(\"data/tcga_sample/expression.tsv\", sep=\"\\t\")\n",
    "survival = pd.read_csv(\"data/tcga_sample/survival.tsv\", sep=\"\\t\", skiprows=1, header=None)\n",
    "meta = pd.read_csv(\"data/tcga_sample/metadata.tsv\", sep=\"\\t\", skiprows=1, header=None)\n",
    "cytokines = pd.read_csv(\"data/genes.cytokine_immune.txt\", skiprows=2, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the TPM values for cytokines pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use cytokine expression\n",
    "tpm = tpm.reindex(cytokines.iloc[:,0].unique(), axis='columns')\n",
    "tpm = tpm.dropna(axis=1)\n",
    "\n",
    "# perform quantile normalization\n",
    "# https://stackoverflow.com/questions/37935920/quantile-normalization-on-pandas-dataframe\n",
    "tpm /= np.max(np.abs(tpm),axis=0) # scale between [0,1]\n",
    "rank_mean = tpm.stack().groupby(tpm.rank(method='first').stack().astype(int)).mean()\n",
    "tpm = tpm.rank(method='min').stack().astype(int).map(rank_mean).unstack()\n",
    "\n",
    "# convert pandas df to np array\n",
    "tpm = tpm.values\n",
    "survival = survival.iloc[:,1:3].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "VALIDATION_SPLIT = 0.8\n",
    "\n",
    "# indices = np.arange(tpm.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# # tpm = tpm[indices]\n",
    "# labels = surv_time[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * tpm.shape[0])\n",
    "\n",
    "x_train = tpm[:num_validation_samples]\n",
    "y_train = survival[:num_validation_samples]\n",
    "x_val = tpm[num_validation_samples:]\n",
    "y_val = survival[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def negative_log_partial_likelihood_loss(regularization):\n",
    "    #Wrapper function for the negative logg partial likelihood loss function\n",
    "    \n",
    "    def loss(y_true, risk):\n",
    "        return negative_log_partial_likelihood(y_true, risk, regularization)\n",
    "    return loss\n",
    "\n",
    "def negative_log_partial_likelihood(censor, risk, regularization):\n",
    "    \"\"\"Return the negative log-partial likelihood of the prediction\n",
    "    y_true contains the survival time\n",
    "    risk is the risk output from the neural network\n",
    "    censor is the vector of inputs that are censored\n",
    "    regularization is the regularization constant (not used currently)\n",
    "    \n",
    "    Uses the Keras backend to perform calculations\n",
    "    \n",
    "    Sorts the surv_time by sorted reverse time\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate negative log likelihood from estimated risk\n",
    "    K.print_tensor(censor)\n",
    "    K.print_tensor(risk)\n",
    "    hazard_ratio = K.exp(risk)\n",
    "    log_risk = K.log(tf.cumsum(hazard_ratio)) # cumsum on sorted surv time accounts for concordance\n",
    "    uncensored_likelihood = risk - log_risk\n",
    "    censored_likelihood = uncensored_likelihood * censor\n",
    "    num_observed_events = K.sum(censor)\n",
    "    neg_likelihood = - K.sum(censored_likelihood) / tf.cast(num_observed_events, tf.float32)\n",
    "    return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "negative_log_partial_likelihood(y_train, np.random.rand(y_train.shape[0], 2), 0).eval(session=K.get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00770439, 0.00770439, 0.00770439, 0.04726486, 0.02192631,\n",
       "         0.00770439, 0.00770439, 0.04556465, 0.03650854, 0.00770439,\n",
       "         0.03650854, 0.03388218, 0.00770439, 0.00770439, 0.00770439,\n",
       "         0.00770439, 0.0255335 , 0.02985164, 0.01795834, 0.03388218,\n",
       "         0.03103552, 0.06391589, 0.00770439, 0.02082614, 0.03144983,\n",
       "         0.04844985, 0.02192631, 0.01385125, 0.0357079 , 0.03144983,\n",
       "         0.12206171],\n",
       "        [0.00770439, 0.00770439, 0.10555878, 0.06691886, 0.03300299,\n",
       "         0.28954815, 0.00770439, 0.04073407, 0.05937713, 0.2058534 ,\n",
       "         0.04073407, 0.03750752, 0.00770439, 0.00770439, 0.00770439,\n",
       "         0.00770439, 0.03103552, 0.04644952, 0.0793616 , 0.03878618,\n",
       "         0.05328115, 0.11175108, 0.12206171, 0.06691886, 0.05433715,\n",
       "         0.04556465, 0.0701573 , 0.12903708, 0.03388218, 0.05433715,\n",
       "         0.02430823],\n",
       "        [0.00770439, 0.10290051, 1.        , 0.23850831, 0.23850831,\n",
       "         0.23850831, 0.09256924, 0.31530727, 0.61459652, 0.00770439,\n",
       "         0.2058534 , 0.02874648, 0.09777909, 0.00770439, 0.00770439,\n",
       "         0.1833617 , 0.08782074, 0.06533576, 0.48187445, 0.07715581,\n",
       "         0.48187445, 0.13392274, 0.28954815, 1.        , 0.0502553 ,\n",
       "         0.05104396, 1.        , 0.15091908, 0.15091908, 1.        ,\n",
       "         0.03465731],\n",
       "        [0.00770439, 0.00770439, 0.00770439, 0.01549208, 0.09507118,\n",
       "         0.03465731, 0.17495124, 0.03232818, 0.01108299, 0.00770439,\n",
       "         0.01108299, 0.01108299, 0.14483557, 1.        , 1.        ,\n",
       "         0.23850831, 0.0357079 , 0.03465731, 0.05937713, 0.06885928,\n",
       "         0.05937713, 0.02430823, 0.00770439, 0.02145167, 0.01049853,\n",
       "         0.00770439, 0.01385125, 0.10876231, 0.12903708, 0.02287782,\n",
       "         0.01049853],\n",
       "        [0.00770439, 0.00770439, 0.03232818, 0.02375978, 0.0125152 ,\n",
       "         0.0357079 , 0.00770439, 0.04921721, 0.02611727, 0.00770439,\n",
       "         0.04844985, 0.02985164, 0.00770439, 0.00770439, 0.00770439,\n",
       "         0.14483557, 0.00770439, 0.01908645, 0.02985164, 0.01795834,\n",
       "         0.01908645, 0.16017084, 0.02813981, 0.00770439, 0.13890046,\n",
       "         0.06691886, 0.01549208, 0.01908645, 0.02375978, 0.01549208,\n",
       "         0.03650854]]), array([[1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_iter(data, labels, batch_size, shuffle=True, isValidationSet=False):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "\n",
    "    # Sorts the batches by survival time\n",
    "    def data_generator():\n",
    "        data_size = len(data)\n",
    "        while True:\n",
    "            # Sample from the dataset for each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                X, y = shuffled_data[start_index: end_index], shuffled_labels[start_index: end_index]\n",
    "                \n",
    "                # Sort X and y by survival time in each batch\n",
    "                idx = np.argsort(abs(y[:,0]))[::-1]\n",
    "                X = X[idx, :]\n",
    "                y = y[idx, 1].reshape(-1,1) # sort by survival time and take censored data\n",
    "\n",
    "                # reshape for matmul\n",
    "                y = y.reshape(-1,1) #reshape to [n, 1] for matmul\n",
    "                \n",
    "                yield X, y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()\n",
    "\n",
    "train_steps, train_batches = batch_iter(x_train, y_train, 5)\n",
    "next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 1s 213ms/step - loss: 2.4145 - acc: 0.3104 - val_loss: 2.0179 - val_acc: 0.1500\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.4459 - acc: 0.3115 - val_loss: 2.0139 - val_acc: 0.1500\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 2.4339 - acc: 0.3198 - val_loss: 2.0103 - val_acc: 0.1500\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.4065 - acc: 0.3115 - val_loss: 2.0054 - val_acc: 0.1500\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 2.3594 - acc: 0.3000 - val_loss: 2.0038 - val_acc: 0.1500\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 2.3581 - acc: 0.2802 - val_loss: 2.0033 - val_acc: 0.1500\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.3403 - acc: 0.2729 - val_loss: 2.0043 - val_acc: 0.1500\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.3381 - acc: 0.2708 - val_loss: 2.0084 - val_acc: 0.1500\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.3755 - acc: 0.2375 - val_loss: 2.0164 - val_acc: 0.1500\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.3305 - acc: 0.2542 - val_loss: 2.0276 - val_acc: 0.1500\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 2.3092 - acc: 0.2302 - val_loss: 2.0435 - val_acc: 0.1500\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.2905 - acc: 0.2771 - val_loss: 2.0607 - val_acc: 0.1500\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.2555 - acc: 0.2500 - val_loss: 2.0793 - val_acc: 0.1500\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.2959 - acc: 0.2656 - val_loss: 2.1012 - val_acc: 0.1500\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.2576 - acc: 0.2115 - val_loss: 2.1258 - val_acc: 0.1500\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.1980 - acc: 0.2688 - val_loss: 2.1560 - val_acc: 0.1500\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 2.2519 - acc: 0.3042 - val_loss: 2.1833 - val_acc: 0.1500\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 2.2238 - acc: 0.2969 - val_loss: 2.2150 - val_acc: 0.2000\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.2053 - acc: 0.2729 - val_loss: 2.2550 - val_acc: 0.1500\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.1849 - acc: 0.2688 - val_loss: 2.2863 - val_acc: 0.1500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1157c3780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=x_train.shape[1], name=\"input\"))\n",
    "model.add(Dense(64, activation='relu', name=\"dense_1\"))\n",
    "model.add(Dropout(0.25, name=\"dropout_1\"))\n",
    "model.add(Dense(64, activation='relu', name=\"dense_2\"))\n",
    "model.add(Dense(1, activation='linear', name=\"output\"))\n",
    "\n",
    "opt = Adam(lr=0.001)\n",
    "\n",
    "model_loss = negative_log_partial_likelihood_loss(0)\n",
    "\n",
    "model.compile(optimizer=opt, loss=model_loss, metrics=['accuracy'])\n",
    "\n",
    "train_steps, train_batches = batch_iter(x_train, y_train, BATCH_SIZE)\n",
    "valid_steps, valid_batches = batch_iter(x_val, y_val, BATCH_SIZE)\n",
    "model.fit_generator(train_batches, train_steps, epochs=20, validation_data=valid_batches, validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
