{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Immunotherapy Response based on RNA-Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data and gene pathways\n",
    "TCGA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tpm = pd.read_csv(\"data/tcga_sample/expression.tsv\", sep=\"\\t\")\n",
    "survival = pd.read_csv(\"data/tcga_sample/survival.tsv\", sep=\"\\t\", skiprows=1, header=None)\n",
    "meta = pd.read_csv(\"data/tcga_sample/metadata.tsv\", sep=\"\\t\", skiprows=1, header=None)\n",
    "cytokines = pd.read_csv(\"data/genes.cytokine_immune.txt\", skiprows=2, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the TPM values for cytokines pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use cytokine expression\n",
    "tpm = tpm.reindex(cytokines.iloc[:,0].unique(), axis='columns')\n",
    "tpm = tpm.dropna(axis=1)\n",
    "\n",
    "# perform quantile normalization\n",
    "# https://stackoverflow.com/questions/37935920/quantile-normalization-on-pandas-dataframe\n",
    "tpm /= np.max(np.abs(tpm),axis=0) # scale between [0,1]\n",
    "rank_mean = tpm.stack().groupby(tpm.rank(method='first').stack().astype(int)).mean()\n",
    "tpm = tpm.rank(method='min').stack().astype(int).map(rank_mean).unstack()\n",
    "\n",
    "# convert pandas df to np array\n",
    "tpm = tpm.values\n",
    "survival = survival.iloc[:,1:3].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "VALIDATION_SPLIT = 0.8\n",
    "\n",
    "# indices = np.arange(tpm.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# # tpm = tpm[indices]\n",
    "# labels = surv_time[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * tpm.shape[0])\n",
    "\n",
    "x_train = tpm[:num_validation_samples]\n",
    "y_train = survival[:num_validation_samples]\n",
    "x_val = tpm[num_validation_samples:]\n",
    "y_val = survival[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "def negative_log_partial_likelihood_loss(regularization):\n",
    "    #Wrapper function for the negative logg partial likelihood loss function\n",
    "    \n",
    "    def loss(y_true, risk):\n",
    "        return negative_log_partial_likelihood(y_true, risk, regularization)\n",
    "    return loss\n",
    "\n",
    "def negative_log_partial_likelihood(censor, risk, regularization):\n",
    "    \"\"\"Return the negative log-partial likelihood of the prediction\n",
    "    y_true contains the survival time\n",
    "    risk is the risk output from the neural network\n",
    "    censor is the vector of inputs that are censored\n",
    "    regularization is the regularization constant (not used currently)\n",
    "    \n",
    "    Uses the Keras backend to perform calculations\n",
    "    \n",
    "    Sorts the surv_time by sorted reverse time\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate negative log likelihood from estimated risk\n",
    "    K.print_tensor(censor)\n",
    "    K.print_tensor(risk)\n",
    "    hazard_ratio = K.exp(risk)\n",
    "    log_risk = K.log(tf.cumsum(hazard_ratio)) # cumsum on sorted surv time accounts for concordance\n",
    "    uncensored_likelihood = risk - log_risk\n",
    "    censored_likelihood = uncensored_likelihood * censor\n",
    "    num_observed_events = K.sum(censor)\n",
    "    neg_likelihood = - K.sum(censored_likelihood) / tf.cast(num_observed_events, tf.float32)\n",
    "    return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "negative_log_partial_likelihood(y_train, np.random.rand(y_train.shape[0], 2), 0).eval(session=K.get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00770439, 0.00770439, 0.00770439, 0.03465731, 0.03103552,\n",
       "         0.00770439, 0.00770439, 0.0427311 , 0.19463238, 0.00770439,\n",
       "         0.01795834, 0.00770439, 0.00770439, 0.00770439, 0.00770439,\n",
       "         0.31530727, 0.09777909, 0.01305174, 0.00770439, 0.02611727,\n",
       "         0.03465731, 0.03650854, 0.07452724, 0.0357079 , 0.01908645,\n",
       "         0.00923611, 0.02375978, 0.04186489, 0.02145167, 0.02375978,\n",
       "         0.09507118],\n",
       "        [0.00770439, 0.00770439, 0.15091908, 0.06056726, 0.11175108,\n",
       "         0.0902439 , 0.00770439, 0.07228801, 0.06265707, 0.00770439,\n",
       "         0.06141154, 0.04644952, 0.16017084, 0.00770439, 0.31530727,\n",
       "         0.00770439, 0.04726486, 0.09256924, 0.08175386, 0.06265707,\n",
       "         0.10290051, 0.09507118, 0.0520031 , 0.0793616 , 0.10555878,\n",
       "         0.04356357, 0.12206171, 0.0902439 , 0.08175386, 0.08782074,\n",
       "         0.06533576],\n",
       "        [0.0902439 , 0.16835241, 0.26754767, 0.13392274, 0.03650854,\n",
       "         0.61459652, 0.48187445, 0.09777909, 0.11175108, 0.31530727,\n",
       "         0.12903708, 0.17495124, 0.2058534 , 0.61459652, 0.00770439,\n",
       "         0.17495124, 0.28954815, 0.26754767, 0.36083847, 0.10555878,\n",
       "         0.13392274, 0.04921721, 0.23850831, 0.19463238, 0.26754767,\n",
       "         0.06056726, 0.17495124, 0.26754767, 0.0701573 , 0.13392274,\n",
       "         0.2058534 ],\n",
       "        [0.17495124, 0.00770439, 0.00770439, 0.01385125, 0.01996093,\n",
       "         0.0502553 , 0.00770439, 0.02611727, 0.01622714, 0.00770439,\n",
       "         0.03144983, 0.07452724, 0.13890046, 0.00770439, 0.00770439,\n",
       "         0.00770439, 0.02375978, 0.07452724, 0.04186489, 0.0502553 ,\n",
       "         0.02813981, 0.10290051, 0.00770439, 0.03103552, 0.01549208,\n",
       "         0.61459652, 0.01049853, 0.02082614, 0.03300299, 0.02985164,\n",
       "         0.04477338],\n",
       "        [0.19463238, 0.00770439, 0.00770439, 0.00770439, 0.36083847,\n",
       "         0.00770439, 0.07452724, 0.06141154, 0.04556465, 0.00770439,\n",
       "         0.13392274, 0.06056726, 0.00770439, 0.00770439, 0.00770439,\n",
       "         0.00770439, 0.01996093, 0.01622714, 0.01908645, 0.01549208,\n",
       "         0.0125152 , 0.11855542, 0.0502553 , 0.00923611, 0.09507118,\n",
       "         0.03388218, 0.02287782, 0.03232818, 0.03465731, 0.01049853,\n",
       "         0.03103552]]), array([[0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_iter(data, labels, batch_size, shuffle=True, isValidationSet=False):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "\n",
    "    # Sorts the batches by survival time\n",
    "    def data_generator():\n",
    "        data_size = len(data)\n",
    "        while True:\n",
    "            # Sample from the dataset for each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                X, y = shuffled_data[start_index: end_index], shuffled_labels[start_index: end_index]\n",
    "                \n",
    "                # Sort X and y by survival time in each batch\n",
    "                idx = np.argsort(abs(y[:,0]))[::-1]\n",
    "                X = X[idx, :]\n",
    "                y = y[idx, 1].reshape(-1,1) # sort by survival time and take censored data\n",
    "\n",
    "                # reshape for matmul\n",
    "                y = y.reshape(-1,1) #reshape to [n, 1] for matmul\n",
    "                \n",
    "                yield X, y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()\n",
    "\n",
    "train_steps, train_batches = batch_iter(x_train, y_train, 5)\n",
    "next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 2.3793 - acc: 0.3031 - val_loss: 2.0015 - val_acc: 0.1500\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.3794 - acc: 0.3073 - val_loss: 2.0007 - val_acc: 0.1500\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.3733 - acc: 0.3240 - val_loss: 1.9991 - val_acc: 0.1500\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.3983 - acc: 0.3083 - val_loss: 2.0001 - val_acc: 0.1500\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.4015 - acc: 0.2969 - val_loss: 2.0021 - val_acc: 0.1500\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 2.2621 - acc: 0.2625 - val_loss: 2.0072 - val_acc: 0.1500\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.3477 - acc: 0.2771 - val_loss: 2.0144 - val_acc: 0.1500\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.3009 - acc: 0.2854 - val_loss: 2.0253 - val_acc: 0.1500\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.3054 - acc: 0.2688 - val_loss: 2.0379 - val_acc: 0.1500\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.3171 - acc: 0.3010 - val_loss: 2.0513 - val_acc: 0.1500\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.2880 - acc: 0.2573 - val_loss: 2.0679 - val_acc: 0.1000\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.3292 - acc: 0.2469 - val_loss: 2.0834 - val_acc: 0.1000\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.2947 - acc: 0.2344 - val_loss: 2.1033 - val_acc: 0.1000\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.2476 - acc: 0.2917 - val_loss: 2.1213 - val_acc: 0.1000\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.2234 - acc: 0.2229 - val_loss: 2.1363 - val_acc: 0.1000\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.2375 - acc: 0.2427 - val_loss: 2.1497 - val_acc: 0.1500\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.2237 - acc: 0.1688 - val_loss: 2.1670 - val_acc: 0.1500\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.1900 - acc: 0.1813 - val_loss: 2.1848 - val_acc: 0.1500\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.2365 - acc: 0.2281 - val_loss: 2.2100 - val_acc: 0.1500\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.2030 - acc: 0.2385 - val_loss: 2.2416 - val_acc: 0.1500\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=x_train.shape[1], name=\"input\"))\n",
    "model.add(Dense(64, activation='relu', name=\"dense_1\"))\n",
    "model.add(Dropout(0.25, name=\"dropout_1\"))\n",
    "model.add(Dense(64, activation='relu', name=\"dense_2\"))\n",
    "model.add(Dense(1, activation='linear', name=\"output\"))\n",
    "\n",
    "opt = Adam(lr=0.001)\n",
    "\n",
    "model_loss = negative_log_partial_likelihood_loss(0)\n",
    "\n",
    "model.compile(optimizer=opt, loss=model_loss, metrics=['accuracy']) # Accuracy is meaningless in this case, only look at loss\n",
    "\n",
    "train_steps, train_batches = batch_iter(x_train, y_train, BATCH_SIZE)\n",
    "valid_steps, valid_batches = batch_iter(x_val, y_val, BATCH_SIZE)\n",
    "history = model.fit_generator(train_batches, train_steps, epochs=20, validation_data=valid_batches, validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11257906],\n",
       "       [-0.5241337 ],\n",
       "       [-0.46911052],\n",
       "       [-0.08380042],\n",
       "       [-0.5498078 ],\n",
       "       [ 0.05152579],\n",
       "       [ 0.1644065 ],\n",
       "       [ 0.00339719],\n",
       "       [ 0.10040384],\n",
       "       [-1.3693784 ],\n",
       "       [ 0.2524556 ],\n",
       "       [-0.13181654],\n",
       "       [-0.29819685],\n",
       "       [-2.2470195 ],\n",
       "       [-0.5133749 ],\n",
       "       [-0.38345382],\n",
       "       [-0.41970626],\n",
       "       [ 0.4906529 ],\n",
       "       [-0.9174869 ],\n",
       "       [ 0.3788603 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(x_val)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5321637426900585"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "\n",
    "predictions_time = np.exp(predictions)\n",
    "concordance_index(y_val[:,0], predictions_time, y_val[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
